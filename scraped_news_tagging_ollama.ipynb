{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf0a9f9d",
   "metadata": {},
   "source": [
    "- Yake 키워드 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4ea439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yake\n",
    "import ollama\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_keywords_yake(input_file, output_file, max_keywords=5):\n",
    "    kw_extractor = yake.KeywordExtractor(top=max_keywords, stopwords=None)\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, \\\n",
    "         open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in infile:\n",
    "            try:\n",
    "                entry = json.loads(line.strip())\n",
    "                text = (entry.get('title', '') + \" \" + entry.get('description', '')).strip()\n",
    "                if text:\n",
    "                    keywords = [kw for kw, score in kw_extractor.extract_keywords(text)]\n",
    "                else:\n",
    "                    keywords = []\n",
    "                entry['keywords'] = keywords\n",
    "                json.dump(entry, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Error parsing JSON line: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing entry: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d45603",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fe8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2: Generating tags via Ollama...\n"
     ]
    }
   ],
   "source": [
    "# Controlled vocabulary\n",
    "controlled_vocab = {\n",
    "    'org': ['OpenAI', 'Anthropic', 'Naver', 'Google', 'Microsoft', 'NVIDIA', 'MIT', 'Facebook', 'Apple', 'Intel', 'Sony', 'Honeywell', 'Oracle', 'SenseTime'],\n",
    "    'model': ['GPT-6', 'Claude-3.7', 'Genie', 'Assistant', 'Azure', 'Mini Cheetah', 'Smart Compose'],\n",
    "    'domain': ['Healthcare', 'Fintech', 'Education', 'Transportation', 'Robotics'],\n",
    "    'topic': ['Multimodal', 'RAG', 'Agents', 'Safety', 'Robotics'],\n",
    "    'event': ['NeurIPS2025', 'GoogleIO', 'WWDC', 'MAX'],\n",
    "    'geo': ['KR', 'US', 'EU', 'CN'],\n",
    "    'biz': ['M&A', 'Funding', 'Earnings', 'Pricing', 'Hiring'],\n",
    "    'policy': ['Regulation', 'Standard', 'Grant']\n",
    "}\n",
    "\n",
    "def get_tags_with_ollama(title, content, yake_keywords, vocab, model_name=\"gemma3:4b\"):\n",
    "    vocab_text = \"\\n\".join([f\"{k}: {', '.join(v)}\" for k, v in vocab.items()])\n",
    "    yake_text = \", \".join(yake_keywords)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "You are an expert tagger for AI-related articles. Your task is to generate relevant tags in the format 'category/keyword' based on the provided controlled vocabulary and YAKE keywords.\n",
    "\n",
    "**Controlled Vocabulary**:\n",
    "{vocab_text}\n",
    "\n",
    "**YAKE Keywords**:\n",
    "{yake_text}\n",
    "\n",
    "**Rules**:\n",
    "1. Prioritize tags from the controlled vocabulary when the title or content matches exactly or closely.\n",
    "2. If a YAKE keyword or content term doesn't match the vocabulary but is relevant, propose a new tag within allowed categories.\n",
    "3. Capitalize keywords in tags for consistency.\n",
    "4. Output only comma-separated tags in 'category/Keyword'.\n",
    "\n",
    "**Article**:\n",
    "Title: {title}\n",
    "Content: {content}\n",
    "\n",
    "**Output**:\n",
    "\"\"\"\n",
    "    try:\n",
    "        response = ollama.chat(model=model_name, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "        raw_tags = response['message']['content'].strip()\n",
    "        tag_list = [t.strip() for t in raw_tags.split(\",\") if t.strip()]\n",
    "        return tag_list\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama: {e}\")\n",
    "        return []\n",
    "\n",
    "# Worker function for a batch\n",
    "def process_batch(batch):\n",
    "    updated_entries = []\n",
    "    # Add tqdm for processing entries within a batch\n",
    "    for entry in tqdm(batch, desc=\"Tagging entries\", leave=False):\n",
    "        title = entry.get('title', '')\n",
    "        content = entry.get('description', '')\n",
    "        keywords = entry.get('keywords', [])\n",
    "        tags = get_tags_with_ollama(title, content, keywords, controlled_vocab)\n",
    "        entry['tags'] = tags\n",
    "        updated_entries.append(entry)\n",
    "    return updated_entries\n",
    "\n",
    "# Main parallel processing function\n",
    "def generate_tags_parallel(input_file, output_file, batch_size=5, max_workers=2):\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        all_entries = [json.loads(line.strip()) for line in f if line.strip()]\n",
    "    \n",
    "    batches = [all_entries[i:i+batch_size] for i in range(0, len(all_entries), batch_size)]\n",
    "    results = []\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        # Add tqdm for batch processing\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            try:\n",
    "                results.extend(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a batch: {e}\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for entry in results:\n",
    "            json.dump(entry, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"Final JSONL with tags saved to: {output_file}\")\n",
    "\n",
    "# ------------------ Main Execution ------------------\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    raw_input_file = r\".\\Data\\Input\\all_entries_20250825_025249 (1).jsonl\"\n",
    "    keyworded_file = r\".\\Data\\Output\\articles_with_keywords.jsonl\"\n",
    "    final_tagged_file = r\".\\Data\\Output\\all_entries_tags.jsonl\"\n",
    "\n",
    "    #print(\"Step 1: Extracting YAKE keywords...\")\n",
    "    #extract_keywords_yake(raw_input_file, keyworded_file, max_keywords=5)\n",
    "\n",
    "    print(\"Step 2: Generating tags via Ollama...\")\n",
    "    generate_tags_parallel(keyworded_file, final_tagged_file, batch_size=5, max_workers=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
