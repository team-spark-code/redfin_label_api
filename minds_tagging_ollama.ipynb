{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb55f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try UTF-8 first\n",
    "try:\n",
    "    df = pd.read_csv(\".\\Data\\ai_news_large.csv\", encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    # Fallback to latin-1 if utf-8 fails\n",
    "    df = pd.read_csv(\".\\Data\\ai_news_large.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92e566a",
   "metadata": {},
   "source": [
    " - TF-IDF version 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c27e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Google discontinues Clips, the AI-powered came...   \n",
      "1  AI can help doctors spot brain hemorrhages faster   \n",
      "2  Pentagon's draft AI ethics guidelines fight bi...   \n",
      "3  Google Says New AI-Powered Search Update Is 'H...   \n",
      "4  Sonar drone helps find a WWII Japanese aircraf...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  While Google was busy showcasing its latest de...   \n",
      "1  AI is already capable of discovering medical c...   \n",
      "2  Tech companies might have trouble establishing...   \n",
      "3  Google is injecting its search engine with new...   \n",
      "4  The late Paul Allen's underwater robotics are ...   \n",
      "\n",
      "                                            keywords  \n",
      "0  clips, camera, google, users meant, promised r...  \n",
      "1  brain hemorrhages, hemorrhages, brain, accurac...  \n",
      "2  trouble establishing, trouble, companies troub...  \n",
      "3  search, google, says new, queries handles, des...  \n",
      "4  carrier, japanese, aircraft, aircraft carrier,...  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Combine title + abstract\n",
    "df['text_for_keywords'] = df['Title'] + \" \" + df['Abstract']\n",
    "\n",
    "# Fit TF-IDF on the whole corpus\n",
    "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,2))  # unigrams + bigrams\n",
    "tfidf_matrix = vectorizer.fit_transform(df['text_for_keywords'])\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Function to get top N keywords per article\n",
    "def get_top_keywords_per_article(row_vec, feature_names, top_n=5):\n",
    "    row = row_vec.toarray().flatten()\n",
    "    top_indices = row.argsort()[-top_n:][::-1]  # indices of top scores\n",
    "    return [feature_names[i] for i in top_indices]\n",
    "\n",
    "# Apply to each row\n",
    "df['keywords'] = [\", \".join(get_top_keywords_per_article(tfidf_matrix[i], feature_names, top_n=5))\n",
    "                  for i in range(tfidf_matrix.shape[0])]\n",
    "\n",
    "# Preview\n",
    "print(df[['Title', 'Abstract', 'keywords']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83b677a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                          clips, camera, google, users meant, promised record\n",
      "1           brain hemorrhages, hemorrhages, brain, accuracy, radiologists test\n",
      "2      trouble establishing, trouble, companies trouble, rogue, pentagon draft\n",
      "3             search, google, says new, queries handles, described significant\n",
      "4             carrier, japanese, aircraft, aircraft carrier, japanese aircraft\n",
      "                                        ...                                   \n",
      "142       opioid, drug diversion, stealing drugs, stealing, pharmacists nurses\n",
      "143                           alphabet, boston dynamics, boston, dynamics, day\n",
      "144                 steps, seeking, searchers seeking, searchers, simple steps\n",
      "145                        multi planet, multi, planet systems, planet, kepler\n",
      "146                                  pixel, pixel like, older, compact, phones\n",
      "Name: keywords, Length: 147, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Set display option to show full content\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now print the Series\n",
    "print(df['keywords'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46a867",
   "metadata": {},
   "source": [
    "- Yake 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccd6670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 Title  \\\n",
      "0    Google discontinues Clips, the AI-powered camera you forgot about   \n",
      "1                    AI can help doctors spot brain hemorrhages faster   \n",
      "2  Pentagon's draft AI ethics guidelines fight bias and rogue machines   \n",
      "3      Google Says New AI-Powered Search Update Is 'Huge Step Forward'   \n",
      "4              Sonar drone helps find a WWII Japanese aircraft carrier   \n",
      "\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                               Abstract  \\\n",
      "0  While Google was busy showcasing its latest devices yesterday, it was also, more quietly, pulling the plug on a few others. Today, it confirmed that it has removed its Clips camera from the Google Store. Google Clips was a short-lived camera that users were meant to position around their homes. It had artificially intelligent auto-capture and promised to record clips when it saw something interesting, like something cute your kid or pet might do...   \n",
      "1     AI is already capable of discovering medical conditions with a high degree of accuracy. However, brain hemorrhages are particularly challenging -- false positives slow things down, while missing even a tiny hemorrhage could be deadly. The technology might be ready for it, however. UC Berkeley and UCSF researchers have created an algorithm that detected brain hemorrhages with accuracy better than two out of four radiologists in a test. The key...   \n",
      "2                                                                                                                                                                                                                                                                                                                         Tech companies might have trouble establishing groundwork for the ethical use of AI, but the Defense Department appears to be moving forward.   \n",
      "3                                                                                                                                                                                                                                            Google is injecting its search engine with new technology to better interpret the billions of web queries it handles every day, a change top executives described as one of the most significant in the company's history.   \n",
      "4     The late Paul Allen's underwater robotics are still achieving firsts in discovering long-lost warships. Vulcan's research vessel Petrel and its two robotic vehicles have discovered the Kaga, a Japanese aircraft carrier sunk during WWII's pivotal Battle of Midway. It's the first time anyone has found a Japanese carrier, Vulcan said, and also the most extensive search the Petrel team has conducted. The team spent several weeks combing an entire...   \n",
      "\n",
      "                                                                                                                    keywords_str  \n",
      "0                      latest devices yesterday, Google discontinues Clips, devices yesterday, pulling the plug, busy showcasing  \n",
      "1             discovering medical conditions, doctors spot brain, brain hemorrhages faster, spot brain hemorrhages, doctors spot  \n",
      "2  rogue machines Tech, machines Tech companies, ethics guidelines fight, guidelines fight bias, trouble establishing groundwork  \n",
      "3                                    Huge Step Forward, AI-Powered Search Update, change top executives, Huge Step, Step Forward  \n",
      "4      Paul Allen underwater, late Paul Allen, Allen underwater robotics, discovering long-lost warships, WWII Japanese aircraft  \n"
     ]
    }
   ],
   "source": [
    "## yake \n",
    "import pandas as pd\n",
    "import yake\n",
    "\n",
    "# Load MIND dataset CSV\n",
    "df['text_for_keywords'] = df['Title'] + \" \" + df['Abstract']\n",
    "\n",
    "\n",
    "# YAKE settings\n",
    "language = \"en\"\n",
    "max_ngram_size = 3  # can extract up to 3-word phrases\n",
    "num_keywords = 5    # top 5 keywords per article\n",
    "dedup_threshold = 0.9  # avoid very similar keywords\n",
    "\n",
    "# Initialize YAKE extractor\n",
    "yake_extractor = yake.KeywordExtractor(lan=language,\n",
    "                                       n=max_ngram_size,\n",
    "                                       dedupLim=dedup_threshold,\n",
    "                                       top=num_keywords,\n",
    "                                       features=None)\n",
    "\n",
    "# Extract keywords per article\n",
    "df['keywords'] = df['text_for_keywords'].apply(lambda x: [kw for kw, score in yake_extractor.extract_keywords(x)])\n",
    "\n",
    "# Optionally convert to comma-separated string\n",
    "df['keywords_str'] = df['keywords'].apply(lambda kws: \", \".join(kws))\n",
    "\n",
    "# Preview\n",
    "print(df[['Title', 'Abstract', 'keywords_str']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eecc881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                          [latest devices yesterday, Google discontinues Clips, devices yesterday, pulling the plug, busy showcasing]\n",
       "1                 [discovering medical conditions, doctors spot brain, brain hemorrhages faster, spot brain hemorrhages, doctors spot]\n",
       "2      [rogue machines Tech, machines Tech companies, ethics guidelines fight, guidelines fight bias, trouble establishing groundwork]\n",
       "3                                        [Huge Step Forward, AI-Powered Search Update, change top executives, Huge Step, Step Forward]\n",
       "4          [Paul Allen underwater, late Paul Allen, Allen underwater robotics, discovering long-lost warships, WWII Japanese aircraft]\n",
       "                                                                    ...                                                               \n",
       "142                                         [Big Tech tackles, diversion opioid crisis, Big Tech, drug diversion opioid, Tech tackles]\n",
       "143          [performing assistive tasks, Boston Dynamics, Everyday Robot Project, rebooted robotics program, robotics program starts]\n",
       "144                           [Withstand Google Constant, Google Constant Updates, Simple Steps, Steps to Withstand, Withstand Google]\n",
       "145      [Kepler Space Telescope, Systems Kepler Discovered, Wild Animation Shows, Multi-Planet Systems Kepler, Space Telescope found]\n",
       "146                                    [Assistant starts reaching, Assistant upgrades, starts reaching older, Assistant starts, Pixel]\n",
       "Name: keywords, Length: 147, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6c337",
   "metadata": {},
   "source": [
    "- Ollama 와 Instruction table (from Notion) 이용한 태깅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "\n",
    "# ---------------- Controlled Vocabulary ----------------\n",
    "controlled_vocab = {\n",
    "    'org': ['OpenAI', 'Anthropic', 'Naver', 'Google', 'Microsoft', 'NVIDIA', 'MIT', 'Facebook', 'Apple', 'Intel', 'Sony', 'Honeywell', 'Oracle', 'SenseTime'],\n",
    "    'model': ['GPT-6', 'Claude-3.7', 'Genie', 'Assistant', 'Azure', 'Mini Cheetah', 'Smart Compose'],\n",
    "    'domain': ['Healthcare', 'Fintech', 'Education', 'Transportation', 'Robotics'],\n",
    "    'topic': ['Multimodal', 'RAG', 'Agents', 'Safety', 'Robotics'],\n",
    "    'event': ['NeurIPS2025', 'GoogleIO', 'WWDC', 'MAX'],\n",
    "    'geo': ['KR', 'US', 'EU', 'CN'],\n",
    "    'biz': ['M&A', 'Funding', 'Earnings', 'Pricing', 'Hiring'],\n",
    "    'policy': ['Regulation', 'Standard', 'Grant']\n",
    "}\n",
    "\n",
    "# ---------------- Ollama tagging function ----------------\n",
    "def get_tags_with_ollama(title, content, yake_keywords, vocab):\n",
    "    vocab_text = \"\\n\".join([f\"{k}: {', '.join(v)}\" for k, v in vocab.items()])\n",
    "    yake_text = \", \".join(yake_keywords)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert tagger for AI-related articles. Your task is to generate relevant tags in the format 'category/keyword' based on the provided controlled vocabulary and YAKE keywords.\n",
    "\n",
    "**Controlled Vocabulary**:\n",
    "{vocab_text}\n",
    "\n",
    "**YAKE Keywords** (for additional context):\n",
    "{yake_text}\n",
    "\n",
    "**Rules**:\n",
    "1. Prioritize tags from the controlled vocabulary when the title or content matches exactly or closely.\n",
    "2. If a YAKE keyword or content term doesn't match the vocabulary but is relevant, propose a new tag within allowed categories.\n",
    "3. Capitalize keywords in tags for consistency.\n",
    "4. Output only the tags, comma-separated, in the format 'category/Keyword'.\n",
    "\n",
    "**Article**:\n",
    "Title: {title}\n",
    "Content: {content}\n",
    "\n",
    "**Output**:\n",
    "Comma-separated tags in the format 'category/Keyword'\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"gemma2:latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        tags_text = response[\"message\"][\"content\"].strip()\n",
    "        tags = [t.strip() for t in tags_text.split(\",\") if t.strip()]\n",
    "        return tags\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama for '{title}': {e}\")\n",
    "        return []\n",
    "\n",
    "# ---------------- Process a batch ----------------\n",
    "def process_batch(batch_rows):\n",
    "    batch_tags = []\n",
    "    for _, row in batch_rows.iterrows():\n",
    "        title = row['Title']\n",
    "        abstract = row['Abstract']\n",
    "        yake_kw = row.get('keywords', [])\n",
    "        if isinstance(yake_kw, str):\n",
    "            try:\n",
    "                yake_kw = eval(yake_kw)\n",
    "            except:\n",
    "                yake_kw = yake_kw.split(\",\")\n",
    "        tags = get_tags_with_ollama(title, abstract, yake_kw, controlled_vocab)\n",
    "        batch_tags.append(tags)\n",
    "    return batch_tags\n",
    "\n",
    "# ---------------- Parallel batch processing ----------------\n",
    "def process_dataframe_parallel(df, batch_size=5, max_workers=2):\n",
    "    # Split df into batches\n",
    "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    all_tags = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=\"Processing batches\"):\n",
    "            try:\n",
    "                all_tags.extend(future.result())\n",
    "            except Exception as e:\n",
    "                print(f\"Batch processing error: {e}\")\n",
    "\n",
    "    df['tags'] = all_tags\n",
    "    return df\n",
    "\n",
    "# ---------------- Usage ----------------\n",
    "# df = pd.read_csv(\"your_file.csv\")  # load your dataframe\n",
    "df = process_dataframe_parallel(df, batch_size=5, max_workers=2)\n",
    "\n",
    "print(df[['Title', 'tags']].head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
