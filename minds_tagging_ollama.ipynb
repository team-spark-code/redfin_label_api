{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb55f5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try UTF-8 first\n",
    "try:\n",
    "    df = pd.read_csv(r\".\\Data\\Input\\ai_news_large.csv\", encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    # Fallback to latin-1 if utf-8 fails\n",
    "    df = pd.read_csv(r\".\\Data\\Input\\ai_news_large.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46a867",
   "metadata": {},
   "source": [
    "- Yake 키워드 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccd6670e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Google discontinues Clips, the AI-powered came...   \n",
      "1  AI can help doctors spot brain hemorrhages faster   \n",
      "2  Pentagon's draft AI ethics guidelines fight bi...   \n",
      "3  Google Says New AI-Powered Search Update Is 'H...   \n",
      "4  Sonar drone helps find a WWII Japanese aircraf...   \n",
      "\n",
      "                                            Abstract  \\\n",
      "0  While Google was busy showcasing its latest de...   \n",
      "1  AI is already capable of discovering medical c...   \n",
      "2  Tech companies might have trouble establishing...   \n",
      "3  Google is injecting its search engine with new...   \n",
      "4  The late Paul Allen's underwater robotics are ...   \n",
      "\n",
      "                                        keywords_str  \n",
      "0  latest devices yesterday, Google discontinues ...  \n",
      "1  discovering medical conditions, doctors spot b...  \n",
      "2  rogue machines Tech, machines Tech companies, ...  \n",
      "3  Huge Step Forward, AI-Powered Search Update, c...  \n",
      "4  Paul Allen underwater, late Paul Allen, Allen ...  \n"
     ]
    }
   ],
   "source": [
    "## yake \n",
    "import pandas as pd\n",
    "import yake\n",
    "\n",
    "# Load MIND dataset CSV\n",
    "df['text_for_keywords'] = df['Title'] + \" \" + df['Abstract']\n",
    "\n",
    "\n",
    "# YAKE settings\n",
    "language = \"en\"\n",
    "max_ngram_size = 3  # can extract up to 3-word phrases\n",
    "num_keywords = 5    # top 5 keywords per article\n",
    "dedup_threshold = 0.9  # avoid very similar keywords\n",
    "\n",
    "# Initialize YAKE extractor\n",
    "yake_extractor = yake.KeywordExtractor(lan=language,\n",
    "                                       n=max_ngram_size,\n",
    "                                       dedupLim=dedup_threshold,\n",
    "                                       top=num_keywords,\n",
    "                                       features=None)\n",
    "\n",
    "# Extract keywords per article\n",
    "df['keywords'] = df['text_for_keywords'].apply(lambda x: [kw for kw, score in yake_extractor.extract_keywords(x)])\n",
    "\n",
    "# Optionally convert to comma-separated string\n",
    "df['keywords_str'] = df['keywords'].apply(lambda kws: \", \".join(kws))\n",
    "\n",
    "# Preview\n",
    "print(df[['Title', 'Abstract', 'keywords_str']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5eecc881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [latest devices yesterday, Google discontinues...\n",
       "1      [discovering medical conditions, doctors spot ...\n",
       "2      [rogue machines Tech, machines Tech companies,...\n",
       "3      [Huge Step Forward, AI-Powered Search Update, ...\n",
       "4      [Paul Allen underwater, late Paul Allen, Allen...\n",
       "                             ...                        \n",
       "142    [Big Tech tackles, diversion opioid crisis, Bi...\n",
       "143    [performing assistive tasks, Boston Dynamics, ...\n",
       "144    [Withstand Google Constant, Google Constant Up...\n",
       "145    [Kepler Space Telescope, Systems Kepler Discov...\n",
       "146    [Assistant starts reaching, Assistant upgrades...\n",
       "Name: keywords, Length: 147, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['keywords']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6c337",
   "metadata": {},
   "source": [
    "- Ollama 와 Instruction table (from Notion) 이용한 태깅 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6680c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import ollama\n",
    "\n",
    "# ---------------- Controlled Vocabulary ----------------\n",
    "controlled_vocab = {\n",
    "    'org': ['OpenAI', 'Anthropic', 'Naver', 'Google', 'Microsoft', 'NVIDIA', 'MIT', 'Facebook', 'Apple', 'Intel', 'Sony', 'Honeywell', 'Oracle', 'SenseTime'],\n",
    "    'model': ['GPT-6', 'Claude-3.7', 'Genie', 'Assistant', 'Azure', 'Mini Cheetah', 'Smart Compose'],\n",
    "    'domain': ['Healthcare', 'Fintech', 'Education', 'Transportation', 'Robotics'],\n",
    "    'topic': ['Multimodal', 'RAG', 'Agents', 'Safety', 'Robotics'],\n",
    "    'event': ['NeurIPS2025', 'GoogleIO', 'WWDC', 'MAX'],\n",
    "    'geo': ['KR', 'US', 'EU', 'CN'],\n",
    "    'biz': ['M&A', 'Funding', 'Earnings', 'Pricing', 'Hiring'],\n",
    "    'policy': ['Regulation', 'Standard', 'Grant']\n",
    "}\n",
    "\n",
    "# ---------------- Ollama tagging function ----------------\n",
    "def get_tags_with_ollama(title, content, yake_keywords, vocab):\n",
    "    vocab_text = \"\\n\".join([f\"{k}: {', '.join(v)}\" for k, v in vocab.items()])\n",
    "    yake_text = \", \".join(yake_keywords)\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert tagger for AI-related articles. Your task is to generate relevant tags in the format 'category/keyword' based on the provided controlled vocabulary and YAKE keywords.\n",
    "\n",
    "**Controlled Vocabulary**:\n",
    "{vocab_text}\n",
    "\n",
    "**YAKE Keywords** (for additional context):\n",
    "{yake_text}\n",
    "\n",
    "**Rules**:\n",
    "1. Prioritize tags from the controlled vocabulary when the title or content matches exactly or closely.\n",
    "2. If a YAKE keyword or content term doesn't match the vocabulary but is relevant, propose a new tag within allowed categories.\n",
    "3. Capitalize keywords in tags for consistency.\n",
    "4. Output only the tags, comma-separated, in the format 'category/Keyword'.\n",
    "\n",
    "**Article**:\n",
    "Title: {title}\n",
    "Content: {content}\n",
    "\n",
    "**Output**:\n",
    "Comma-separated tags in the format 'category/Keyword'\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        response = ollama.chat(\n",
    "            model=\"gemma2:latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        tags_text = response[\"message\"][\"content\"].strip()\n",
    "        tags = [t.strip() for t in tags_text.split(\",\") if t.strip()]\n",
    "        return tags\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling Ollama for '{title}': {e}\")\n",
    "        return []\n",
    "\n",
    "# ---------------- Process a batch ----------------\n",
    "def process_batch(batch_rows):\n",
    "    batch_tags = []\n",
    "    # Initialize tqdm for batch with row count in description\n",
    "    for _, row in tqdm(batch_rows.iterrows(), total=len(batch_rows), desc=f\"Tagging {len(batch_rows)} articles\"):\n",
    "        title = row['Title']\n",
    "        abstract = row['Abstract']\n",
    "        yake_kw = row.get('keywords', [])\n",
    "        if isinstance(yake_kw, str):\n",
    "            try:\n",
    "                yake_kw = eval(yake_kw)\n",
    "            except:\n",
    "                yake_kw = yake_kw.split(\",\")\n",
    "        tags = get_tags_with_ollama(title, abstract, yake_kw, controlled_vocab)\n",
    "        batch_tags.append(tags)\n",
    "    return batch_tags\n",
    "\n",
    "# ---------------- Parallel batch processing ----------------\n",
    "def process_dataframe_parallel(df, batch_size=5, max_workers=2):\n",
    "    # Split df into batches\n",
    "    batches = [df.iloc[i:i+batch_size] for i in range(0, len(df), batch_size)]\n",
    "    all_tags = []\n",
    "    total_rows_processed = 0\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_batch, batch): batch for batch in batches}\n",
    "        # Initialize tqdm for batches with total row count\n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Processing {len(df)} rows in {len(batches)} batches\"):\n",
    "            try:\n",
    "                batch_tags = future.result()\n",
    "                all_tags.extend(batch_tags)\n",
    "                total_rows_processed += len(futures[future])\n",
    "                # Update tqdm description to show rows processed\n",
    "                tqdm.write(f\"Processed {total_rows_processed}/{len(df)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Batch processing error: {e}\")\n",
    "\n",
    "    df['tags'] = all_tags\n",
    "    return df\n",
    "\n",
    "# ---------------- Usage ----------------\n",
    "# Sample DataFrame for demonstration\n",
    "\n",
    "# Process the DataFrame\n",
    "df = process_dataframe_parallel(df, batch_size=2, max_workers=2)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nResults:\")\n",
    "print(df[['Title', 'tags']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3934f0fd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mData\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mOutput\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mai_news_large_tagged.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv(r\".\\Data\\Output\\ai_news_large_tagged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303b0ec1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
