{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b7924f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to output.jsonl\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>guid</th>\n",
       "      <th>source</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>pub_date</th>\n",
       "      <th>description</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>tags</th>\n",
       "      <th>group</th>\n",
       "      <th>scraped_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://openai.com/index/accelerating-life-sci...</td>\n",
       "      <td>OpenAI Blog (공식, 변경 가능성 주의)</td>\n",
       "      <td>Accelerating life sciences research</td>\n",
       "      <td>https://openai.com/index/accelerating-life-sci...</td>\n",
       "      <td>Fri, 22 Aug 2025 08:30:00 GMT</td>\n",
       "      <td>Discover how a specialized AI model, GPT-4b mi...</td>\n",
       "      <td></td>\n",
       "      <td>Research</td>\n",
       "      <td>[org/OpenAI, model/GPT-4b, topic/Robotics, dom...</td>\n",
       "      <td>frontier_lab</td>\n",
       "      <td>2025-08-25 02:51:21.590956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                guid  \\\n",
       "0  https://openai.com/index/accelerating-life-sci...   \n",
       "\n",
       "                        source                                title  \\\n",
       "0  OpenAI Blog (공식, 변경 가능성 주의)  Accelerating life sciences research   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://openai.com/index/accelerating-life-sci...   \n",
       "\n",
       "                        pub_date  \\\n",
       "0  Fri, 22 Aug 2025 08:30:00 GMT   \n",
       "\n",
       "                                         description author  category  \\\n",
       "0  Discover how a specialized AI model, GPT-4b mi...         Research   \n",
       "\n",
       "                                                tags         group  \\\n",
       "0  [org/OpenAI, model/GPT-4b, topic/Robotics, dom...  frontier_lab   \n",
       "\n",
       "                  scraped_at  \n",
       "0 2025-08-25 02:51:21.590956  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ollama\n",
    "import json\n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "\n",
    "# -----------------------------\n",
    "# Classify a batch of articles\n",
    "# -----------------------------\n",
    "def classify_batch(df_chunk):\n",
    "    articles_text = \"\"\n",
    "    for i, row in df_chunk.iterrows():\n",
    "        # Convert Series to dict for safe access\n",
    "        row_dict = row.to_dict()\n",
    "        tags_str = \"\"\n",
    "        if isinstance(row_dict.get(\"tags\", \"\"), list):\n",
    "            tags_str = \", \".join(str(t) for t in row_dict[\"tags\"])\n",
    "        elif isinstance(row_dict.get(\"tags\", \"\"), str):\n",
    "            tags_str = row_dict[\"tags\"]\n",
    "\n",
    "        articles_text += f\"\"\"\n",
    "{i+1}. Title: \"{row_dict['title']}\"\n",
    "     Abstract: \"{row_dict['description']}\"\n",
    "     Keywords: \"{tags_str}\"\n",
    "\"\"\"\n",
    "\n",
    "    # Full prompt (no shortening)\n",
    "    prompt = f\"\"\"\n",
    "You are a professional news analyst. \n",
    "Your task is to classify each news article into **exactly one primary category** \n",
    "out of the six predefined categories below. \n",
    "\n",
    "Use both the **title**, **abstract**, and **keywords** to decide, \n",
    "and follow the **priority rules** when multiple aspects are present.  \n",
    "\n",
    "---\n",
    "\n",
    "### Categories and Rules:\n",
    "\n",
    "1. Research (학술)\n",
    "- 포함: 논문, 프리프린트, 학회 채택/수상, 벤치마크·데이터셋 공개.\n",
    "- 제외: 제품 릴리스 노트(→ Technology).\n",
    "- 경계 규칙: “학회/논문 성과가 리드”면 Research 우선.\n",
    "\n",
    "2. Technology & Product (기술/제품)\n",
    "- 포함: 모델/제품 릴리스, 성능 업데이트, 모델 카드/리드미 변경, 기능 개선.\n",
    "- 제외: 자금/거래 중심 기사(→ Market & Corporate).\n",
    "- 경계 규칙: “연구 성과”보다 “제품·기능” 전달이 리드면 여기.\n",
    "\n",
    "3. Market & Corporate (시장/기업)\n",
    "- 포함: 투자·M&A·IPO·실적, 리더십/조직개편, 제휴·상용화 계약, 상업 로드맵.\n",
    "- 제외: 공공 규제·지원(→ Policy & Regulation).\n",
    "- 경계 규칙: 금액·거래·실적·지배구조가 리드면 여기.\n",
    "\n",
    "4. Policy & Regulation (정책/규제)\n",
    "- 포함: 법·규제·가이드라인, 공공자금(보조금·RFP), 수출통제, 표준화·거버넌스.\n",
    "- 제외: 기업 자체 정책(가격·상업 전략, → Market & Corporate).\n",
    "- 경계 규칙: 공공 주체의 룰/지원이 핵심이면 여기.\n",
    "\n",
    "5. Society & Culture (사회/문화)\n",
    "- 포함: 대중 활용 트렌드, 창작·교육·밈, 저작권/윤리 공론의 사회적 논의(정책화 전 단계).\n",
    "- 제외: 순수 기술 업데이트(→ Technology), 입법·규제(→ Policy).\n",
    "- 경계 규칙: “사회적 파급·수용”이 리드면 여기.\n",
    "\n",
    "6. Incidents & Safety (사건/안전/운영)\n",
    "- 포함: 서비스 장애, 보안 사고/유출, 모델 오남용/중대한 안전 이슈, 리콜/중단.\n",
    "- 제외: 일반 제품 릴리스(→ Technology), 법·제재(→ Policy).\n",
    "- 경계 규칙: “사건/사건/리스크 대응”이 중심이면 여기.\n",
    "\n",
    "---\n",
    "\n",
    "### Priority Rules (apply in order if overlaps):\n",
    "1. Incidents & Safety\n",
    "2. Policy & Regulation\n",
    "3. Market & Corporate\n",
    "4. Research\n",
    "5. Technology & Product\n",
    "6. Society & Culture\n",
    "\n",
    "---\n",
    "\n",
    "### Output format:\n",
    "Return ONLY a valid JSON list, with no extra text, like this:\n",
    "[\n",
    "  {{\"id\": 1, \"category\": \"Research\"}},\n",
    "  {{\"id\": 2, \"category\": \"Policy & Regulation\"}}\n",
    "]\n",
    "\n",
    "Here are the articles:\n",
    "{articles_text}\n",
    "\"\"\"\n",
    "\n",
    "    # Call Ollama\n",
    "    response = ollama.chat(\n",
    "        model=\"gemma3:4b\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "\n",
    "    content = response[\"message\"][\"content\"].strip()\n",
    "\n",
    "    # Extract JSON safely\n",
    "    try:\n",
    "        result_list = json.loads(content)\n",
    "        categories = [r[\"category\"] for r in result_list]\n",
    "        return categories\n",
    "    except json.JSONDecodeError:\n",
    "        categories = re.findall(r'\"category\"\\s*:\\s*\"([^\"]+)\"', content)\n",
    "        if len(categories) == len(df_chunk):\n",
    "            return categories\n",
    "        return [\"Uncategorized\"] * len(df_chunk)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Helper to split dataframe into batches\n",
    "# -----------------------------\n",
    "def chunkify(df, chunk_size=5):\n",
    "    return [df.iloc[i:i+chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Threaded batch classification with tqdm\n",
    "# -----------------------------\n",
    "def classify_dataframe_batch_parallel_thread(df, chunk_size=5, max_workers=2):\n",
    "    chunks = chunkify(df, chunk_size)\n",
    "    categories = []\n",
    "\n",
    "    # Use tqdm to wrap the chunks for progress tracking\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Map classify_batch to chunks and track progress with tqdm\n",
    "        results = list(tqdm(executor.map(classify_batch, chunks), total=len(chunks), desc=\"Classifying articles\"))\n",
    "        for r in results:\n",
    "            categories.extend(r)\n",
    "\n",
    "    df[\"category\"] = categories\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Single-row test helper\n",
    "# -----------------------------\n",
    "def classify_single_row(df, row_index=0):\n",
    "    row = df.iloc[row_index]\n",
    "    category = classify_batch(pd.DataFrame([row]))[0]  # returns list\n",
    "    print(f\"Title: {row['title']}\")\n",
    "    print(f\"Predicted category: {category}\")\n",
    "    return category\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Main function to process JSONL file\n",
    "# -----------------------------\n",
    "def process_jsonl(input_file, output_file=r\".\\data\\output\\all_entries_categories.jsonl\"):\n",
    "    # Load JSONL from file\n",
    "    try:\n",
    "        df = pd.read_json(input_file, lines=True)\n",
    "    except ValueError as e:\n",
    "        print(f\"Error reading JSONL file: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Classify with progress bar\n",
    "    df = classify_dataframe_batch_parallel_thread(df)\n",
    "    \n",
    "    # Output to JSONL file\n",
    "    df.to_json(output_file, orient='records', lines=True)\n",
    "    print(f\"output saved to {output_file}\")\n",
    "    return df.head()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "jsonl_file = r\".\\data\\output\\all_entries_keywords.jsonl\"\n",
    "process_jsonl(jsonl_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_recom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
