{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60a2dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/workspace/redfin/redfin_label_api/notebooks\n"
     ]
    }
   ],
   "source": [
    "# pip install yake rake-nltk keybert sentence-transformers networkx langdetect scikit-learn pandas numpy python-dotenv\n",
    "from __future__ import annotations\n",
    "import json, random, re, os, time, math\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langdetect import detect\n",
    "import networkx as nx\n",
    "\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62609010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == CONFIG ==\n",
    "SAMPLE_SIZE        = 100              # ìƒ˜í”Œ ê°œìˆ˜\n",
    "STRATIFY_FIELD     = \"source\"         # ì¸µí™” ìƒ˜í”Œë§ ê¸°ì¤€(ì—†ìœ¼ë©´ None)\n",
    "ID_FIELD           = \"_id\"            # ë¬¸ì„œ ê³ ìœ  ID í•„ë“œ\n",
    "TEXT_FIELDS        = [\"title\", \"summary\", \"body_text\"]  # ë¼ë²¨ëŸ¬ì—ê²Œ ë³´ì—¬ì¤„ í…ìŠ¤íŠ¸\n",
    "TOP_K_CANDIDATES   = 12               # ì œì•ˆ í‚¤ì›Œë“œ ê°œìˆ˜\n",
    "RANDOM_SEED        = 42\n",
    "\n",
    "# ì…ì¶œë ¥ ê²½ë¡œ\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"keywords\"\n",
    "INPUT_JSONL        = DATA_DIR / \"articles.jsonl\"  # ì›ë³¸ ìˆ˜ì§‘ ë°ì´í„°(JSONL)\n",
    "TEMPLATE_CSV       = DATA_DIR / \"gold_template.csv\"        # ë¼ë²¨ë§ í…œí”Œë¦¿(ë‹¨ì¼ annotatorìš©)\n",
    "TEMPLATE_A_CSV     = DATA_DIR / \"gold_template_A.csv\"      # annotator A ë°°í¬ìš©\n",
    "TEMPLATE_B_CSV     = DATA_DIR / \"gold_template_B.csv\"      # annotator B ë°°í¬ìš©\n",
    "MERGED_CSV         = DATA_DIR / \"gold_merged.csv\"          # A/B ë¼ë²¨ í•©ì¹œ ë’¤ ì ê²€ìš©\n",
    "FINAL_GOLD_JSON    = DATA_DIR / \"gold_keywords.json\"       # ìµœì¢… ê³¨ë“œì…‹ {id: [kw,...]}\n",
    "\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893c5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(ln))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    return rows\n",
    "\n",
    "def concat_text(row: pd.Series, fields: list[str]) -> str:\n",
    "    parts = []\n",
    "    for k in fields:\n",
    "        v = row[k] if k in row else \"\"\n",
    "        if pd.isna(v):\n",
    "            continue\n",
    "        s = v if isinstance(v, str) else str(v)\n",
    "        s = s.strip()\n",
    "        if s and s.lower() != \"nan\":\n",
    "            parts.append(s)\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "def stratified_sample(df: pd.DataFrame, by: str, n: int) -> pd.DataFrame:\n",
    "    if not by or by not in df.columns:\n",
    "        return df.sample(n=min(n, len(df)), random_state=RANDOM_SEED)\n",
    "    # ì¸µë³„ ê· ë“± ë¹„ìœ¨ ìƒ˜í”Œ (ì¸µ ìˆ˜ì— ë”°ë¼ ëª«/ë‚˜ë¨¸ì§€ ë¶„ë°°)\n",
    "    groups = df.groupby(by)\n",
    "    per = max(1, n // max(1, groups.ngroups))\n",
    "    out = groups.apply(lambda g: g.sample(n=min(per, len(g)), random_state=RANDOM_SEED)).reset_index(drop=True)\n",
    "    if len(out) < n:\n",
    "        remain = n - len(out)\n",
    "        pool = df.drop(out.index, errors=\"ignore\")\n",
    "        extra = pool.sample(n=min(remain, len(pool)), random_state=RANDOM_SEED)\n",
    "        out = pd.concat([out, extra], ignore_index=True)\n",
    "    return out.head(min(n, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2427f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_n_from_jsonl(path: str, n: int) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if len(rows) >= n:\n",
    "                break\n",
    "                \n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                rows.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ê²½ê³ : {line_num}ë²ˆì§¸ ì¤„ JSON íŒŒì‹± ì‹¤íŒ¨ - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"ì´ {len(rows)}ê°œì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\")\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4a90896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_extract_jsonl(path: str = None) -> List[Dict[str, Any]]:\n",
    "    if path is None:\n",
    "        path = str(INPUT_JSONL)\n",
    "    \n",
    "    # íŒŒì¼ ì¡´ì¬ í™•ì¸\n",
    "    if not Path(path).exists():\n",
    "        print(f\"ì˜¤ë¥˜: íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤ - {path}\")\n",
    "        return []\n",
    "    \n",
    "    # ì‚¬ìš©ì ì…ë ¥ ë°›ê¸°\n",
    "    try:\n",
    "        n = int(input(f\"ì¶”ì¶œí•  ë°ì´í„° ê°œìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 10): \") or \"10\")\n",
    "        if n <= 0:\n",
    "            print(\"ì˜¤ë¥˜: ì–‘ìˆ˜ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "            return []\n",
    "    except ValueError:\n",
    "        print(\"ì˜¤ë¥˜: ìˆ«ìë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "        return []\n",
    "    \n",
    "    # ë°ì´í„° ì¶”ì¶œ\n",
    "    print(f\"\\n{path}ì—ì„œ ì²˜ìŒ {n}ê°œ ë°ì´í„°ë¥¼ ì¶”ì¶œ ì¤‘...\")\n",
    "    data = extract_first_n_from_jsonl(path, n)\n",
    "    \n",
    "    if data:\n",
    "        print(f\"\\nì¶”ì¶œ ì™„ë£Œ! ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "        for i, item in enumerate(data[:3], 1):\n",
    "            print(f\"{i}. ID: {item.get('_id', 'N/A')}\")\n",
    "            print(f\"   Title: {item.get('title', 'N/A')[:80]}...\")\n",
    "            print(f\"   Source: {item.get('source', 'N/A')}\")\n",
    "            print()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# ê°„í¸ ì‚¬ìš©ì„ ìœ„í•œ ë˜í¼ í•¨ìˆ˜ë“¤\n",
    "def get_first_10(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"ì²˜ìŒ 10ê°œ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 10)\n",
    "\n",
    "def get_first_50(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"ì²˜ìŒ 50ê°œ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 50)\n",
    "\n",
    "def get_first_100(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"ì²˜ìŒ 100ê°œ ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ab7eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 10ê°œì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\n",
      "ì´ 50ê°œì˜ ë°ì´í„°ë¥¼ ì¶”ì¶œí–ˆìŠµë‹ˆë‹¤.\n",
      "goldset_10.jsonl, goldset_50.jsonl íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ\n",
      "\n",
      "=== ë°©ë²• 3: ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ ===\n",
      "\n",
      "=== ë°©ë²• 4: ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ===\n",
      "interactive_extract_jsonl() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.\n",
      "ì˜ˆ: data = interactive_extract_jsonl()\n",
      "\n",
      "DataFrame í˜•íƒœë¡œ ë³€í™˜ ì™„ë£Œ:\n",
      "ì»¬ëŸ¼: ['_id', 'guid', 'source', 'title', 'link', 'pub_date', 'author', 'category', 'tags', 'group', 'scraped_at', 'body_text', 'body_text_length', 'summary']\n",
      "í–‰ ìˆ˜: 25\n",
      "\n",
      "ì²« 3í–‰ ë¯¸ë¦¬ë³´ê¸°:\n",
      "                        _id  \\\n",
      "0  68b7e70933df1b522ba3be0a   \n",
      "1  68b7e70933df1b522ba3be0b   \n",
      "2  68b7e70933df1b522ba3be0c   \n",
      "\n",
      "                                               title  \\\n",
      "0                Accelerating life sciences research   \n",
      "1  Scaling domain expertise in complex, regulated...   \n",
      "2         Mixi reimagines communication with ChatGPT   \n",
      "\n",
      "                        source  \\\n",
      "0  OpenAI Blog (ê³µì‹, ë³€ê²½ ê°€ëŠ¥ì„± ì£¼ì˜)   \n",
      "1  OpenAI Blog (ê³µì‹, ë³€ê²½ ê°€ëŠ¥ì„± ì£¼ì˜)   \n",
      "2  OpenAI Blog (ê³µì‹, ë³€ê²½ ê°€ëŠ¥ì„± ì£¼ì˜)   \n",
      "\n",
      "                                           body_text  body_text_length  \\\n",
      "0  August 22, 2025 OpenAI and Retro Biosciences a...             14059   \n",
      "1  August 21, 2025 Blue J scaled its AI-powered t...              6071   \n",
      "2  August 20, 2025 Discover how MIXI deployed Cha...              5706   \n",
      "\n",
      "                                             summary category tags  \n",
      "0  OpenAI and Retro Biosciences achieve 50x incre...            []  \n",
      "1  Blue J has scaled its AI-powered tax research ...            []  \n",
      "2  MIXI deployed ChatGPT Enterprise in just 45 da...            []  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample_10 = get_first_10()\n",
    "sample_50 = get_first_50()\n",
    "\n",
    "def save_jsonl(data: list[dict], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(sample_10, PROJECT_ROOT / \"data\" / \"keywords\" / \"goldset_10.jsonl\")\n",
    "save_jsonl(sample_50, PROJECT_ROOT / \"data\" / \"keywords\" / \"goldset_50.jsonl\")\n",
    "print(\"goldset_10.jsonl, goldset_50.jsonl íŒŒì¼ë¡œ ì €ì¥ ì™„ë£Œ\")\n",
    "\n",
    "\n",
    "# ë°©ë²• 3: ë‹¤ë¥¸ JSONL íŒŒì¼ì—ì„œ ì¶”ì¶œ\n",
    "print(\"\\n=== ë°©ë²• 3: ë‹¤ë¥¸ íŒŒì¼ì—ì„œ ì¶”ì¶œ ===\")\n",
    "# ë‹¤ë¥¸ JSONL íŒŒì¼ì´ ìˆë‹¤ë©´ ê²½ë¡œë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "# other_data = extract_first_n_from_jsonl(\"ë‹¤ë¥¸íŒŒì¼.jsonl\", 30)\n",
    "\n",
    "# ë°©ë²• 4: ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ (ì£¼í”¼í„°ì—ì„œëŠ” input()ì´ ì œí•œì ì¼ ìˆ˜ ìˆìŒ)\n",
    "print(\"\\n=== ë°©ë²• 4: ì¸í„°ë™í‹°ë¸Œ ëª¨ë“œ ===\")\n",
    "print(\"interactive_extract_jsonl() í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ì‚¬ìš©ì ì…ë ¥ì„ ë°›ìŠµë‹ˆë‹¤.\")\n",
    "print(\"ì˜ˆ: data = interactive_extract_jsonl()\")\n",
    "\n",
    "# ì¶”ì¶œëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜í•˜ì—¬ í™•ì¸\n",
    "if first_25:\n",
    "    df_sample = pd.DataFrame(first_25)\n",
    "    print(f\"\\nDataFrame í˜•íƒœë¡œ ë³€í™˜ ì™„ë£Œ:\")\n",
    "    print(f\"ì»¬ëŸ¼: {list(df_sample.columns)}\")\n",
    "    print(f\"í–‰ ìˆ˜: {len(df_sample)}\")\n",
    "    print(f\"\\nì²« 3í–‰ ë¯¸ë¦¬ë³´ê¸°:\")\n",
    "    print(df_sample[['_id', 'title', 'source', 'body_text', 'body_text_length', 'summary', 'category', 'tags']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631e8b3",
   "metadata": {},
   "source": [
    "### ì•Œê³ ë¦¬ì¦˜ë³„ í‚¤ì›Œë“œ ì¶”ì¶œ\n",
    "\n",
    "ê³µí†µ íŒŒë¼ë¯¸í„°:\n",
    "- top_k = 10\n",
    "- YAKE: n<=3, dedupLim=0.9\n",
    "- RAKE: max_length=3\n",
    "- TextRank: ìœˆë„ìš°=2, ë‹¨ì–´ê·¸ë˜í”„\n",
    "- KeyBERT: all-MiniLM-L6-v2, use_maximum=True, nr_candidates=20, diversity=0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd93b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# ê²½ëŸ‰ ì„ë² ë”©(ì˜ë¬¸): CPU ê°€ëŠ¥\n",
    "_EMBED = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "_KB = KeyBERT(model=_EMBED)\n",
    "\n",
    "# 1. YAKE\n",
    "def yake_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    ex = yake.KeywordExtractor(lan=\"en\", n=3, top=k, dedupLim=0.9, windowsSize=2)\n",
    "    return [kw for kw,_ in ex.extract_keywords(text)]\n",
    "\n",
    "# 2. RAKE\n",
    "def rake_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    return [kw for kw,_ in Rake.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                                top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                                diversity=0.6)]\n",
    "\n",
    "\n",
    "# 3. TextRank\n",
    "def textrank_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    return [kw for kw,_ in TextRank.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                                    top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                                    diversity=0.6)]\n",
    "\n",
    "# 4. KeyBERT\n",
    "def keybert_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    pairs = _KB.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                 top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                 diversity=0.6)\n",
    "    return [kw for kw,_ in pairs]\n",
    "\n",
    "def make_candidates(text: str, k: int = TOP_K_CANDIDATES) -> Dict[str, List[str]]:\n",
    "    if not text:\n",
    "        return {\"yake\": [], \"keybert\": []}\n",
    "    return {\n",
    "        \"yake\": yake_candidates(text, k=k),\n",
    "        \"keybert\": keybert_candidates(text, k=k),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a6237f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# í›„ë³´ ìƒì„±(ë¼ë²¨ëŸ¬ ë³´ì¡°)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cands = \u001b[43msample_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisplay_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_CANDIDATES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_yake\u001b[39m\u001b[33m\"\u001b[39m]   = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     14\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_keybert\u001b[39m\u001b[33m\"\u001b[39m] = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m      9\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# í›„ë³´ ìƒì„±(ë¼ë²¨ëŸ¬ ë³´ì¡°)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cands = sample_df[\u001b[33m\"\u001b[39m\u001b[33mdisplay_text\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mmake_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_CANDIDATES\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     13\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_yake\u001b[39m\u001b[33m\"\u001b[39m]   = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     14\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_keybert\u001b[39m\u001b[33m\"\u001b[39m] = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmake_candidates\u001b[39m\u001b[34m(text, k)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m: yake_candidates(text, k=k),\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mkeybert_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     25\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mkeybert_candidates\u001b[39m\u001b[34m(text, k)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeybert_candidates\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = TOP_K_CANDIDATES) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     pairs = \u001b[43m_KB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyphrase_ngram_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_maxsum\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdiversity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw,_ \u001b[38;5;129;01min\u001b[39;00m pairs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/keybert/_model.py:235\u001b[39m, in \u001b[36mKeyBERT.extract_keywords\u001b[39m\u001b[34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Max Sum Distance\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m use_maxsum:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     keywords = \u001b[43mmax_sum_distance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnr_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Cosine-based keyword extraction\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/keybert/_maxsum.py:51\u001b[39m, in \u001b[36mmax_sum_distance\u001b[39m\u001b[34m(doc_embedding, word_embeddings, words, top_n, nr_candidates)\u001b[39m\n\u001b[32m     49\u001b[39m candidate = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m itertools.combinations(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words_idx)), top_n):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     sim = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sim < min_sim:\n\u001b[32m     53\u001b[39m         candidate = combination\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ì›ë³¸ ë¡œë“œ\n",
    "rows = read_jsonl(INPUT_JSONL)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# í‘œì‹œìš© í…ìŠ¤íŠ¸ êµ¬ì„±\n",
    "df[\"display_text\"] = df.apply(lambda r: concat_text(r, TEXT_FIELDS), axis=1)\n",
    "\n",
    "# ìƒ˜í”Œë§\n",
    "sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "\n",
    "# í›„ë³´ ìƒì„±(ë¼ë²¨ëŸ¬ ë³´ì¡°)\n",
    "cands = sample_df[\"display_text\"].apply(lambda t: make_candidates(t, k=TOP_K_CANDIDATES))\n",
    "sample_df[\"cands_yake\"]   = cands.apply(lambda d: \", \".join(d[\"yake\"]))\n",
    "sample_df[\"cands_keybert\"] = cands.apply(lambda d: \", \".join(d[\"keybert\"]))\n",
    "\n",
    "# ë¼ë²¨ ì…ë ¥ ì»¬ëŸ¼(ë¹„ì›Œì„œ ì œê³µ)\n",
    "sample_df[\"gold_keywords\"] = \"\"         # ë¼ë²¨ëŸ¬ê°€ í¸ì§‘\n",
    "sample_df[\"annotator\"]     = \"\"         # ë¼ë²¨ëŸ¬ ì´ë¦„/ì´ë‹ˆì…œ\n",
    "sample_df[\"notes\"]         = \"\"         # ì£¼ì„/ì˜ê²¬\n",
    "\n",
    "# í…œí”Œë¦¿ì— í•„ìš”í•œ í•µì‹¬ ì»¬ëŸ¼ë§Œ\n",
    "cols = [ID_FIELD, \"guid\", \"source\", \"title\", \"summary\", \"display_text\", \"cands_yake\", \"cands_keybert\", \"gold_keywords\", \"annotator\", \"notes\"]\n",
    "template = sample_df.loc[:, [c for c in cols if c in sample_df.columns]]\n",
    "\n",
    "template.to_csv(TEMPLATE_CSV, index=False)\n",
    "# ë‘ ëª…ì—ê²Œ ë¶„ë°°í•  ë³„ë„ íŒŒì¼ë„ ìƒì„±(ë™ì¼ ë‚´ìš©)\n",
    "template.to_csv(TEMPLATE_A_CSV, index=False)\n",
    "template.to_csv(TEMPLATE_B_CSV, index=False)\n",
    "\n",
    "template.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ab657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == ë¼ë²¨ë§ ìœ„ì ¯ ==\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as W\n",
    "import pandas as pd\n",
    "import os, json, re\n",
    "\n",
    "# ì§„í–‰ ì €ì¥ ê²½ë¡œ (ì¤‘ê°„ ì €ì¥)\n",
    "AUTOSAVE_CSV = \"gold_template_working.csv\"\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame):\n",
    "    for col in (\"gold_keywords\",\"annotator\",\"notes\",\"cands_yake\",\"cands_keybert\",\"display_text\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    return df\n",
    "\n",
    "def _normalize_kw_list(s: str) -> str:\n",
    "    \"\"\"\n",
    "    ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  êµ¬ë¶„ â†’ ì •ê·œí™”(ì†Œë¬¸ì, ê³µë°±ì •ë¦¬, ì¤‘ë³µì œê±°)\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    toks = re.split(r\"[;,]\", s)\n",
    "    toks = [re.sub(r\"\\s+\", \" \", t).strip().lower() for t in toks if t.strip()]\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(t) < 2: \n",
    "            continue\n",
    "        if re.search(r\"https?://\", t):\n",
    "            continue\n",
    "        if t not in out:\n",
    "            out.append(t)\n",
    "    return \", \".join(out)\n",
    "\n",
    "def launch_labeler(df: pd.DataFrame,\n",
    "                   id_field: str = \"_id\",\n",
    "                   show_fields = (\"title\",\"summary\"),\n",
    "                   autosave_csv: str = AUTOSAVE_CSV):\n",
    "    \"\"\"\n",
    "    sample_dfë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ë¼ë²¨ë§í•˜ëŠ” ì¸í„°ë™í‹°ë¸Œ ìœ„ì ¯.\n",
    "    - df: sample_df (cands_yake / cands_keybert / display_text í¬í•¨)\n",
    "    - id_field: ë¬¸ì„œ ê³ ìœ í‚¤ ì»¬ëŸ¼\n",
    "    - show_fields: ìƒë‹¨ì— ë³´ì—¬ì¤„ í•„ë“œë“¤\n",
    "    - autosave_csv: ì´ë™ ì‹œ ìë™ ì €ì¥ë˜ëŠ” ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    assert id_field in df.columns, f\"{id_field} not in dataframe\"\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    df = _ensure_cols(df)\n",
    "\n",
    "    # ì´ì „ ì‘ì—… ì´ì–´ë¶™ì´ê¸°\n",
    "    if os.path.exists(autosave_csv):\n",
    "        try:\n",
    "            prev = pd.read_csv(autosave_csv)\n",
    "            if id_field in prev.columns:\n",
    "                df = df.drop(columns=[c for c in (\"gold_keywords\",\"annotator\",\"notes\") if c in df.columns], errors=\"ignore\") \\\n",
    "                       .merge(prev[[id_field,\"gold_keywords\",\"annotator\",\"notes\"]],\n",
    "                              on=id_field, how=\"left\")\n",
    "                for col in (\"gold_keywords\",\"annotator\",\"notes\"):\n",
    "                    df[col] = df[col].fillna(\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] failed to load previous autosave: {e}\")\n",
    "\n",
    "    N = len(df)\n",
    "    idx = 0\n",
    "\n",
    "    # ---- ìœ„ì ¯ë“¤ ----\n",
    "    idx_disp = W.BoundedIntText(value=1, min=1, max=max(1, N), description=\"Index\")\n",
    "    progress = W.Label()\n",
    "    id_label = W.HTML()\n",
    "\n",
    "    # í‘œì‹œ í…ìŠ¤íŠ¸(ê°€ë…)\n",
    "    show_areas = []\n",
    "    for f in show_fields:\n",
    "        area = W.HTML(layout=W.Layout(border=\"1px solid #ddd\", padding=\"8px\", height=\"auto\"))\n",
    "        show_areas.append((f, area))\n",
    "\n",
    "    # í›„ë³´ ì„ íƒ\n",
    "    def _split_cands(s: str):\n",
    "        return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "    cands_yake_ms   = W.SelectMultiple(options=[], description=\"YAKE\", layout=W.Layout(width=\"48%\", height=\"140px\"))\n",
    "    cands_keybert_ms= W.SelectMultiple(options=[], description=\"KeyBERT\", layout=W.Layout(width=\"48%\", height=\"140px\"))\n",
    "    cands_box = W.HBox([cands_yake_ms, cands_keybert_ms])\n",
    "\n",
    "    # í¸ì§‘ ì˜ì—­\n",
    "    annotator_tb = W.Text(value=\"\", description=\"Annotator\", layout=W.Layout(width=\"40%\"))\n",
    "    notes_ta     = W.Textarea(value=\"\", description=\"Notes\", layout=W.Layout(width=\"100%\", height=\"80px\"))\n",
    "    gold_ta      = W.Textarea(value=\"\", description=\"Gold Keywords (,ë¡œ êµ¬ë¶„)\", layout=W.Layout(width=\"100%\", height=\"110px\"))\n",
    "\n",
    "    # ë²„íŠ¼ë“¤\n",
    "    btn_prev   = W.Button(description=\"â—€ Prev\", button_style=\"\")\n",
    "    btn_next   = W.Button(description=\"Next â–¶\", button_style=\"primary\")\n",
    "    btn_add_y  = W.Button(description=\"Add YAKE â–¶\", button_style=\"\")\n",
    "    btn_add_k  = W.Button(description=\"Add KeyBERT â–¶\", button_style=\"\")\n",
    "    btn_clear  = W.Button(description=\"Clear gold\", button_style=\"warning\")\n",
    "    btn_save   = W.Button(description=\"ğŸ’¾ Save\", button_style=\"success\")\n",
    "\n",
    "    msg = W.HTML()\n",
    "\n",
    "    def _update_view():\n",
    "        i = idx_disp.value - 1\n",
    "        row = df.iloc[i]\n",
    "        # í—¤ë”/ì§„í–‰ë¥ \n",
    "        id_label.value = f\"<b>{id_field}</b>: {row.get(id_field)} | <b>source</b>: {row.get('source','')}\"\n",
    "        progress.value = f\"{idx_disp.value}/{N}\"\n",
    "\n",
    "        # ë³¸ë¬¸ í‘œì‹œ\n",
    "        for f, area in show_areas:\n",
    "            area.value = f\"<b>{f}</b><br>{(row.get(f) or '')}\"\n",
    "\n",
    "        # í›„ë³´ ëª©ë¡\n",
    "        cands_yake_ms.options    = _split_cands(row.get(\"cands_yake\",\"\"))\n",
    "        cands_keybert_ms.options = _split_cands(row.get(\"cands_keybert\",\"\"))\n",
    "\n",
    "        # í¸ì§‘ ê°’\n",
    "        annotator_tb.value = row.get(\"annotator\",\"\") or \"\"\n",
    "        notes_ta.value     = row.get(\"notes\",\"\") or \"\"\n",
    "        gold_ta.value      = row.get(\"gold_keywords\",\"\") or \"\"\n",
    "\n",
    "        msg.value = \"\"\n",
    "\n",
    "    def _write_back(i):\n",
    "        # normalizeí•˜ê³  dfì— ë˜ì“°ê¸°\n",
    "        df.at[i, \"annotator\"]     = annotator_tb.value.strip()\n",
    "        df.at[i, \"notes\"]         = notes_ta.value.strip()\n",
    "        df.at[i, \"gold_keywords\"] = _normalize_kw_list(gold_ta.value)\n",
    "\n",
    "    def _autosave():\n",
    "        try:\n",
    "            df.to_csv(autosave_csv, index=False)\n",
    "            return True, f\"Saved to {autosave_csv}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Save failed: {e}\"\n",
    "\n",
    "    def _add_selected(ms_widget):\n",
    "        cur = _normalize_kw_list(gold_ta.value)\n",
    "        cur_set = set([s.strip() for s in cur.split(\",\") if s.strip()]) if cur else set()\n",
    "        add_set = set([s.lower() for s in ms_widget.value])\n",
    "        merged = list(dict.fromkeys([*cur_set, *add_set]))\n",
    "        gold_ta.value = \", \".join(merged)\n",
    "\n",
    "    # ì´ë²¤íŠ¸ ë°”ì¸ë”©\n",
    "    def on_prev(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i-1 if i>0 else 0)  # í˜„ì¬ ë³€ê²½ì‚¬í•­ë„ ë°˜ì˜ë˜ë„ë¡\n",
    "        ok, m = _autosave()\n",
    "        idx_disp.value = max(1, i)\n",
    "        _update_view()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    def on_next(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i)\n",
    "        ok, m = _autosave()\n",
    "        idx_disp.value = min(N, i+2)\n",
    "        _update_view()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    def on_jump(change):\n",
    "        # ì¸ë±ìŠ¤ ìˆ˜ë™ ë³€ê²½ ì‹œ í˜„ì¬ ë³€ê²½ì‚¬í•­ ì €ì¥\n",
    "        i = max(0, min(N-1, change[\"old\"]-1))\n",
    "        _write_back(i)\n",
    "        _autosave()\n",
    "        _update_view()\n",
    "\n",
    "    def on_add_y(_): _add_selected(cands_yake_ms)\n",
    "    def on_add_k(_): _add_selected(cands_keybert_ms)\n",
    "    def on_clear(_): gold_ta.value = \"\"\n",
    "    def on_save(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i)\n",
    "        ok, m = _autosave()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    btn_prev.on_click(on_prev)\n",
    "    btn_next.on_click(on_next)\n",
    "    btn_add_y.on_click(on_add_y)\n",
    "    btn_add_k.on_click(on_add_k)\n",
    "    btn_clear.on_click(on_clear)\n",
    "    btn_save.on_click(on_save)\n",
    "    idx_disp.observe(on_jump, names=\"value\")\n",
    "\n",
    "    header = W.HBox([idx_disp, progress])\n",
    "    meta   = W.HTML(\"<b>Document Meta</b>\")\n",
    "    cand_btns = W.HBox([btn_add_y, btn_add_k, btn_clear, btn_save])\n",
    "    nav    = W.HBox([btn_prev, btn_next])\n",
    "\n",
    "    # ë ˆì´ì•„ì›ƒ\n",
    "    box = W.VBox([\n",
    "        header, id_label,\n",
    "        W.HTML(\"<hr>\"),\n",
    "        meta,\n",
    "        *(area for _, area in show_areas),\n",
    "        W.HTML(\"<hr><b>Candidate Keywords</b>\"),\n",
    "        cands_box, cand_btns,\n",
    "        W.HTML(\"<hr><b>Gold Label</b>\"),\n",
    "        annotator_tb, gold_ta, notes_ta,\n",
    "        nav, msg\n",
    "    ])\n",
    "\n",
    "    # ì´ˆê¸°í™”\n",
    "    _update_view()\n",
    "    display(box)\n",
    "\n",
    "    # ë°˜í™˜: ì‘ì—…ë³¸ DataFrameì„ í˜¸ì¶œìë„ ì°¸ì¡° ê°€ëŠ¥\n",
    "    return df\n",
    "\n",
    "# ---- ì‚¬ìš© ì˜ˆì‹œ ----\n",
    "# ì•„ë˜ í•œ ì¤„ì„ ì‹¤í–‰í•˜ë©´ ìœ„ì ¯ì´ ëœ¹ë‹ˆë‹¤.\n",
    "# sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "# sample_df = _ensure_cols(sample_df)\n",
    "# labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\"title\",\"summary\",\"display_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a04593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_ensure_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sample_df = \u001b[43m_ensure_cols\u001b[49m(sample_df)  \u001b[38;5;66;03m# ì•ˆì „í•˜ê²Œ ì»¬ëŸ¼ ì¤€ë¹„\u001b[39;00m\n\u001b[32m      3\u001b[39m labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdisplay_text\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name '_ensure_cols' is not defined"
     ]
    }
   ],
   "source": [
    "sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "sample_df = _ensure_cols(sample_df)  # ì•ˆì „í•˜ê²Œ ì»¬ëŸ¼ ì¤€ë¹„\n",
    "labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\"title\",\"summary\",\"display_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‘ ëª…ì˜ CSVë¥¼ ìˆ˜ì§‘ í›„ í•©ì˜ ì»¬ëŸ¼ì„ ë§Œë“­ë‹ˆë‹¤. ê¸°ë³¸ì€ êµì§‘í•©+ë¶€ë¶„í•©ì˜ ê·œì¹™ì„ ì œê³µí•˜ê³ , ë¶ˆì¼ì¹˜ ê±´ì„ í‘œì‹œ\n",
    "def parse_kw_list(s: str) -> List[str]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    # ì‰¼í‘œ/ì„¸ë¯¸ì½œë¡  ê³µìš© íŒŒì‹±, ì†Œë¬¸ì/íŠ¸ë¦¼\n",
    "    toks = re.split(r\"[;,]\", s)\n",
    "    toks = [re.sub(r\"\\s+\", \" \", t).strip().lower() for t in toks if t.strip()]\n",
    "    # ì¤‘ë³µ ì œê±°, ê¸¸ì´ 2 ë¯¸ë§Œ/URL ì œê±°\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(t) < 2: \n",
    "            continue\n",
    "        if re.search(r\"https?://\", t):\n",
    "            continue\n",
    "        if t not in out:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "# A/B ê²°ê³¼ ë¡œë“œ\n",
    "A = pd.read_csv(TEMPLATE_A_CSV)\n",
    "B = pd.read_csv(TEMPLATE_B_CSV)\n",
    "\n",
    "# ë³‘í•© í‚¤: ID_FIELD\n",
    "merged = A.merge(B[[ID_FIELD, \"gold_keywords\"]], on=ID_FIELD, suffixes=(\"_A\", \"_B\"), how=\"outer\")\n",
    "\n",
    "# í•©ì˜ ê·œì¹™\n",
    "final_kw, agree, jacc = [], [], []\n",
    "for _, r in merged.iterrows():\n",
    "    kwsA = parse_kw_list(r.get(\"gold_keywords_A\",\"\"))\n",
    "    kwsB = parse_kw_list(r.get(\"gold_keywords_B\",\"\"))\n",
    "    inter = sorted(set(kwsA) & set(kwsB))\n",
    "    union = sorted(set(kwsA) | set(kwsB))\n",
    "    # ê¸°ë³¸ í•©ì˜ì•ˆ: êµì§‘í•©ì´ 5ê°œ ë¯¸ë§Œì´ë©´ ì¤‘ìš”í•œ í›„ë³´(êµì§‘í•© + A/B ìƒìœ„ 2ê°œì”©)ë¡œ ì œì•ˆ\n",
    "    if len(inter) >= 5:\n",
    "        fused = inter\n",
    "    else:\n",
    "        fused = sorted(set(inter + kwsA[:2] + kwsB[:2]))\n",
    "    final_kw.append(\", \".join(fused))\n",
    "    agree.append(1 if len(inter) >= max(3, min(len(union)//2, 5)) else 0)\n",
    "    jacc.append(jaccard(kwsA, kwsB))\n",
    "\n",
    "merged[\"gold_keywords_final\"] = final_kw\n",
    "merged[\"annotator_agree_flag\"] = agree       # 1=ëŒ€ì²´ë¡œ í•©ì˜, 0=ì¬ê²€í†  ìš”ë§\n",
    "merged[\"annotator_jaccard\"]    = jacc\n",
    "\n",
    "merged.to_csv(MERGED_CSV, index=False)\n",
    "merged.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
