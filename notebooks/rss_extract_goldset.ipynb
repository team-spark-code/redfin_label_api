{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60a2dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/workspace/redfin/redfin_label_api/notebooks\n"
     ]
    }
   ],
   "source": [
    "# pip install yake rake-nltk keybert sentence-transformers networkx langdetect scikit-learn pandas numpy python-dotenv\n",
    "from __future__ import annotations\n",
    "import json, random, re, os, time, math\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from langdetect import detect\n",
    "import networkx as nx\n",
    "\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "62609010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == CONFIG ==\n",
    "SAMPLE_SIZE        = 100              # 샘플 개수\n",
    "STRATIFY_FIELD     = \"source\"         # 층화 샘플링 기준(없으면 None)\n",
    "ID_FIELD           = \"_id\"            # 문서 고유 ID 필드\n",
    "TEXT_FIELDS        = [\"title\", \"summary\", \"body_text\"]  # 라벨러에게 보여줄 텍스트\n",
    "TOP_K_CANDIDATES   = 12               # 제안 키워드 개수\n",
    "RANDOM_SEED        = 42\n",
    "\n",
    "# 입출력 경로\n",
    "PROJECT_ROOT = Path().cwd().parent\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"keywords\"\n",
    "INPUT_JSONL        = DATA_DIR / \"articles.jsonl\"  # 원본 수집 데이터(JSONL)\n",
    "TEMPLATE_CSV       = DATA_DIR / \"gold_template.csv\"        # 라벨링 템플릿(단일 annotator용)\n",
    "TEMPLATE_A_CSV     = DATA_DIR / \"gold_template_A.csv\"      # annotator A 배포용\n",
    "TEMPLATE_B_CSV     = DATA_DIR / \"gold_template_B.csv\"      # annotator B 배포용\n",
    "MERGED_CSV         = DATA_DIR / \"gold_merged.csv\"          # A/B 라벨 합친 뒤 점검용\n",
    "FINAL_GOLD_JSON    = DATA_DIR / \"gold_keywords.json\"       # 최종 골드셋 {id: [kw,...]}\n",
    "\n",
    "random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "893c5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for ln in f:\n",
    "            ln = ln.strip()\n",
    "            if not ln:\n",
    "                continue\n",
    "            try:\n",
    "                rows.append(json.loads(ln))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "    return rows\n",
    "\n",
    "def concat_text(row: pd.Series, fields: list[str]) -> str:\n",
    "    parts = []\n",
    "    for k in fields:\n",
    "        v = row[k] if k in row else \"\"\n",
    "        if pd.isna(v):\n",
    "            continue\n",
    "        s = v if isinstance(v, str) else str(v)\n",
    "        s = s.strip()\n",
    "        if s and s.lower() != \"nan\":\n",
    "            parts.append(s)\n",
    "    return re.sub(r\"\\s+\", \" \", \" \".join(parts)).strip()\n",
    "\n",
    "def stratified_sample(df: pd.DataFrame, by: str, n: int) -> pd.DataFrame:\n",
    "    if not by or by not in df.columns:\n",
    "        return df.sample(n=min(n, len(df)), random_state=RANDOM_SEED)\n",
    "    # 층별 균등 비율 샘플 (층 수에 따라 몫/나머지 분배)\n",
    "    groups = df.groupby(by)\n",
    "    per = max(1, n // max(1, groups.ngroups))\n",
    "    out = groups.apply(lambda g: g.sample(n=min(per, len(g)), random_state=RANDOM_SEED)).reset_index(drop=True)\n",
    "    if len(out) < n:\n",
    "        remain = n - len(out)\n",
    "        pool = df.drop(out.index, errors=\"ignore\")\n",
    "        extra = pool.sample(n=min(remain, len(pool)), random_state=RANDOM_SEED)\n",
    "        out = pd.concat([out, extra], ignore_index=True)\n",
    "    return out.head(min(n, len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2427f5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_first_n_from_jsonl(path: str, n: int) -> List[Dict[str, Any]]:\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if len(rows) >= n:\n",
    "                break\n",
    "                \n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                rows.append(data)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"경고: {line_num}번째 줄 JSON 파싱 실패 - {e}\")\n",
    "                continue\n",
    "    \n",
    "    print(f\"총 {len(rows)}개의 데이터를 추출했습니다.\")\n",
    "    return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4a90896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interactive_extract_jsonl(path: str = None) -> List[Dict[str, Any]]:\n",
    "    if path is None:\n",
    "        path = str(INPUT_JSONL)\n",
    "    \n",
    "    # 파일 존재 확인\n",
    "    if not Path(path).exists():\n",
    "        print(f\"오류: 파일을 찾을 수 없습니다 - {path}\")\n",
    "        return []\n",
    "    \n",
    "    # 사용자 입력 받기\n",
    "    try:\n",
    "        n = int(input(f\"추출할 데이터 개수를 입력하세요 (기본값: 10): \") or \"10\")\n",
    "        if n <= 0:\n",
    "            print(\"오류: 양수를 입력해주세요.\")\n",
    "            return []\n",
    "    except ValueError:\n",
    "        print(\"오류: 숫자를 입력해주세요.\")\n",
    "        return []\n",
    "    \n",
    "    # 데이터 추출\n",
    "    print(f\"\\n{path}에서 처음 {n}개 데이터를 추출 중...\")\n",
    "    data = extract_first_n_from_jsonl(path, n)\n",
    "    \n",
    "    if data:\n",
    "        print(f\"\\n추출 완료! 데이터 미리보기:\")\n",
    "        for i, item in enumerate(data[:3], 1):\n",
    "            print(f\"{i}. ID: {item.get('_id', 'N/A')}\")\n",
    "            print(f\"   Title: {item.get('title', 'N/A')[:80]}...\")\n",
    "            print(f\"   Source: {item.get('source', 'N/A')}\")\n",
    "            print()\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 간편 사용을 위한 래퍼 함수들\n",
    "def get_first_10(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"처음 10개 데이터 추출\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 10)\n",
    "\n",
    "def get_first_50(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"처음 50개 데이터 추출\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 50)\n",
    "\n",
    "def get_first_100(path: str = None) -> List[Dict[str, Any]]:\n",
    "    \"\"\"처음 100개 데이터 추출\"\"\"\n",
    "    return extract_first_n_from_jsonl(path or str(INPUT_JSONL), 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9ab7eb03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 10개의 데이터를 추출했습니다.\n",
      "총 50개의 데이터를 추출했습니다.\n",
      "goldset_10.jsonl, goldset_50.jsonl 파일로 저장 완료\n",
      "\n",
      "=== 방법 3: 다른 파일에서 추출 ===\n",
      "\n",
      "=== 방법 4: 인터랙티브 모드 ===\n",
      "interactive_extract_jsonl() 함수를 호출하면 사용자 입력을 받습니다.\n",
      "예: data = interactive_extract_jsonl()\n",
      "\n",
      "DataFrame 형태로 변환 완료:\n",
      "컬럼: ['_id', 'guid', 'source', 'title', 'link', 'pub_date', 'author', 'category', 'tags', 'group', 'scraped_at', 'body_text', 'body_text_length', 'summary']\n",
      "행 수: 25\n",
      "\n",
      "첫 3행 미리보기:\n",
      "                        _id  \\\n",
      "0  68b7e70933df1b522ba3be0a   \n",
      "1  68b7e70933df1b522ba3be0b   \n",
      "2  68b7e70933df1b522ba3be0c   \n",
      "\n",
      "                                               title  \\\n",
      "0                Accelerating life sciences research   \n",
      "1  Scaling domain expertise in complex, regulated...   \n",
      "2         Mixi reimagines communication with ChatGPT   \n",
      "\n",
      "                        source  \\\n",
      "0  OpenAI Blog (공식, 변경 가능성 주의)   \n",
      "1  OpenAI Blog (공식, 변경 가능성 주의)   \n",
      "2  OpenAI Blog (공식, 변경 가능성 주의)   \n",
      "\n",
      "                                           body_text  body_text_length  \\\n",
      "0  August 22, 2025 OpenAI and Retro Biosciences a...             14059   \n",
      "1  August 21, 2025 Blue J scaled its AI-powered t...              6071   \n",
      "2  August 20, 2025 Discover how MIXI deployed Cha...              5706   \n",
      "\n",
      "                                             summary category tags  \n",
      "0  OpenAI and Retro Biosciences achieve 50x incre...            []  \n",
      "1  Blue J has scaled its AI-powered tax research ...            []  \n",
      "2  MIXI deployed ChatGPT Enterprise in just 45 da...            []  \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample_10 = get_first_10()\n",
    "sample_50 = get_first_50()\n",
    "\n",
    "def save_jsonl(data: list[dict], path: str) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in data:\n",
    "            f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "save_jsonl(sample_10, PROJECT_ROOT / \"data\" / \"keywords\" / \"goldset_10.jsonl\")\n",
    "save_jsonl(sample_50, PROJECT_ROOT / \"data\" / \"keywords\" / \"goldset_50.jsonl\")\n",
    "print(\"goldset_10.jsonl, goldset_50.jsonl 파일로 저장 완료\")\n",
    "\n",
    "\n",
    "# 방법 3: 다른 JSONL 파일에서 추출\n",
    "print(\"\\n=== 방법 3: 다른 파일에서 추출 ===\")\n",
    "# 다른 JSONL 파일이 있다면 경로를 지정할 수 있습니다\n",
    "# other_data = extract_first_n_from_jsonl(\"다른파일.jsonl\", 30)\n",
    "\n",
    "# 방법 4: 인터랙티브 모드 (주피터에서는 input()이 제한적일 수 있음)\n",
    "print(\"\\n=== 방법 4: 인터랙티브 모드 ===\")\n",
    "print(\"interactive_extract_jsonl() 함수를 호출하면 사용자 입력을 받습니다.\")\n",
    "print(\"예: data = interactive_extract_jsonl()\")\n",
    "\n",
    "# 추출된 데이터를 DataFrame으로 변환하여 확인\n",
    "if first_25:\n",
    "    df_sample = pd.DataFrame(first_25)\n",
    "    print(f\"\\nDataFrame 형태로 변환 완료:\")\n",
    "    print(f\"컬럼: {list(df_sample.columns)}\")\n",
    "    print(f\"행 수: {len(df_sample)}\")\n",
    "    print(f\"\\n첫 3행 미리보기:\")\n",
    "    print(df_sample[['_id', 'title', 'source', 'body_text', 'body_text_length', 'summary', 'category', 'tags']].head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9631e8b3",
   "metadata": {},
   "source": [
    "### 알고리즘별 키워드 추출\n",
    "\n",
    "공통 파라미터:\n",
    "- top_k = 10\n",
    "- YAKE: n<=3, dedupLim=0.9\n",
    "- RAKE: max_length=3\n",
    "- TextRank: 윈도우=2, 단어그래프\n",
    "- KeyBERT: all-MiniLM-L6-v2, use_maximum=True, nr_candidates=20, diversity=0.6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd93b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import yake\n",
    "from rake_nltk import Rake\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "set_seed(42)\n",
    "\n",
    "# 경량 임베딩(영문): CPU 가능\n",
    "_EMBED = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "_KB = KeyBERT(model=_EMBED)\n",
    "\n",
    "# 1. YAKE\n",
    "def yake_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    ex = yake.KeywordExtractor(lan=\"en\", n=3, top=k, dedupLim=0.9, windowsSize=2)\n",
    "    return [kw for kw,_ in ex.extract_keywords(text)]\n",
    "\n",
    "# 2. RAKE\n",
    "def rake_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    return [kw for kw,_ in Rake.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                                top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                                diversity=0.6)]\n",
    "\n",
    "\n",
    "# 3. TextRank\n",
    "def textrank_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    return [kw for kw,_ in TextRank.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                                    top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                                    diversity=0.6)]\n",
    "\n",
    "# 4. KeyBERT\n",
    "def keybert_candidates(text: str, k: int = TOP_K_CANDIDATES) -> List[str]:\n",
    "    pairs = _KB.extract_keywords(text, keyphrase_ngram_range=(1,3),\n",
    "                                 top_n=k, use_maxsum=True, nr_candidates=max(20, k*2),\n",
    "                                 diversity=0.6)\n",
    "    return [kw for kw,_ in pairs]\n",
    "\n",
    "def make_candidates(text: str, k: int = TOP_K_CANDIDATES) -> Dict[str, List[str]]:\n",
    "    if not text:\n",
    "        return {\"yake\": [], \"keybert\": []}\n",
    "    return {\n",
    "        \"yake\": yake_candidates(text, k=k),\n",
    "        \"keybert\": keybert_candidates(text, k=k),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6a6237f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 후보 생성(라벨러 보조)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cands = \u001b[43msample_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisplay_text\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmake_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_CANDIDATES\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_yake\u001b[39m\u001b[33m\"\u001b[39m]   = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     14\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_keybert\u001b[39m\u001b[33m\"\u001b[39m] = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/series.py:4935\u001b[39m, in \u001b[36mSeries.apply\u001b[39m\u001b[34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[39m\n\u001b[32m   4800\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\n\u001b[32m   4801\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   4802\u001b[39m     func: AggFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m   4807\u001b[39m     **kwargs,\n\u001b[32m   4808\u001b[39m ) -> DataFrame | Series:\n\u001b[32m   4809\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4810\u001b[39m \u001b[33;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[32m   4811\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4926\u001b[39m \u001b[33;03m    dtype: float64\u001b[39;00m\n\u001b[32m   4927\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m   4928\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   4930\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4931\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4932\u001b[39m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m=\u001b[49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4933\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4934\u001b[39m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m-> \u001b[39m\u001b[32m4935\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1422\u001b[39m, in \u001b[36mSeriesApply.apply\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1419\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.apply_compat()\n\u001b[32m   1421\u001b[39m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/apply.py:1502\u001b[39m, in \u001b[36mSeriesApply.apply_standard\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[32m   1499\u001b[39m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[32m   1500\u001b[39m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[32m   1501\u001b[39m action = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj.dtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1502\u001b[39m mapped = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[32m   1504\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1506\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[32m0\u001b[39m], ABCSeries):\n\u001b[32m   1507\u001b[39m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[32m   1508\u001b[39m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m obj._constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index=obj.index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/base.py:925\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    923\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m925\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/pandas/core/algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/lib.pyx:2999\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m      9\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# 후보 생성(라벨러 보조)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m cands = sample_df[\u001b[33m\"\u001b[39m\u001b[33mdisplay_text\u001b[39m\u001b[33m\"\u001b[39m].apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mmake_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTOP_K_CANDIDATES\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     13\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_yake\u001b[39m\u001b[33m\"\u001b[39m]   = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m]))\n\u001b[32m     14\u001b[39m sample_df[\u001b[33m\"\u001b[39m\u001b[33mcands_keybert\u001b[39m\u001b[33m\"\u001b[39m] = cands.apply(\u001b[38;5;28;01mlambda\u001b[39;00m d: \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(d[\u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m]))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mmake_candidates\u001b[39m\u001b[34m(text, k)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m text:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m: [], \u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m: []}\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     23\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myake\u001b[39m\u001b[33m\"\u001b[39m: yake_candidates(text, k=k),\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mkeybert\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mkeybert_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     25\u001b[39m }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 14\u001b[39m, in \u001b[36mkeybert_candidates\u001b[39m\u001b[34m(text, k)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mkeybert_candidates\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m, k: \u001b[38;5;28mint\u001b[39m = TOP_K_CANDIDATES) -> List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     pairs = \u001b[43m_KB\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeyphrase_ngram_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_maxsum\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnr_candidates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m*\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m                                 \u001b[49m\u001b[43mdiversity\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.6\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw,_ \u001b[38;5;129;01min\u001b[39;00m pairs]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/keybert/_model.py:235\u001b[39m, in \u001b[36mKeyBERT.extract_keywords\u001b[39m\u001b[34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Max Sum Distance\u001b[39;00m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m use_maxsum:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     keywords = \u001b[43mmax_sum_distance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdoc_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnr_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Cosine-based keyword extraction\u001b[39;00m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/redfin/.venv/lib/python3.12/site-packages/keybert/_maxsum.py:51\u001b[39m, in \u001b[36mmax_sum_distance\u001b[39m\u001b[34m(doc_embedding, word_embeddings, words, top_n, nr_candidates)\u001b[39m\n\u001b[32m     49\u001b[39m candidate = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m combination \u001b[38;5;129;01min\u001b[39;00m itertools.combinations(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(words_idx)), top_n):\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     sim = \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcombination\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m sim < min_sim:\n\u001b[32m     53\u001b[39m         candidate = combination\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 원본 로드\n",
    "rows = read_jsonl(INPUT_JSONL)\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "# 표시용 텍스트 구성\n",
    "df[\"display_text\"] = df.apply(lambda r: concat_text(r, TEXT_FIELDS), axis=1)\n",
    "\n",
    "# 샘플링\n",
    "sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "\n",
    "# 후보 생성(라벨러 보조)\n",
    "cands = sample_df[\"display_text\"].apply(lambda t: make_candidates(t, k=TOP_K_CANDIDATES))\n",
    "sample_df[\"cands_yake\"]   = cands.apply(lambda d: \", \".join(d[\"yake\"]))\n",
    "sample_df[\"cands_keybert\"] = cands.apply(lambda d: \", \".join(d[\"keybert\"]))\n",
    "\n",
    "# 라벨 입력 컬럼(비워서 제공)\n",
    "sample_df[\"gold_keywords\"] = \"\"         # 라벨러가 편집\n",
    "sample_df[\"annotator\"]     = \"\"         # 라벨러 이름/이니셜\n",
    "sample_df[\"notes\"]         = \"\"         # 주석/의견\n",
    "\n",
    "# 템플릿에 필요한 핵심 컬럼만\n",
    "cols = [ID_FIELD, \"guid\", \"source\", \"title\", \"summary\", \"display_text\", \"cands_yake\", \"cands_keybert\", \"gold_keywords\", \"annotator\", \"notes\"]\n",
    "template = sample_df.loc[:, [c for c in cols if c in sample_df.columns]]\n",
    "\n",
    "template.to_csv(TEMPLATE_CSV, index=False)\n",
    "# 두 명에게 분배할 별도 파일도 생성(동일 내용)\n",
    "template.to_csv(TEMPLATE_A_CSV, index=False)\n",
    "template.to_csv(TEMPLATE_B_CSV, index=False)\n",
    "\n",
    "template.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199ab657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# == 라벨링 위젯 ==\n",
    "from IPython.display import display, clear_output\n",
    "import ipywidgets as W\n",
    "import pandas as pd\n",
    "import os, json, re\n",
    "\n",
    "# 진행 저장 경로 (중간 저장)\n",
    "AUTOSAVE_CSV = \"gold_template_working.csv\"\n",
    "\n",
    "def _ensure_cols(df: pd.DataFrame):\n",
    "    for col in (\"gold_keywords\",\"annotator\",\"notes\",\"cands_yake\",\"cands_keybert\",\"display_text\"):\n",
    "        if col not in df.columns:\n",
    "            df[col] = \"\"\n",
    "    return df\n",
    "\n",
    "def _normalize_kw_list(s: str) -> str:\n",
    "    \"\"\"\n",
    "    쉼표/세미콜론 구분 → 정규화(소문자, 공백정리, 중복제거)\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    toks = re.split(r\"[;,]\", s)\n",
    "    toks = [re.sub(r\"\\s+\", \" \", t).strip().lower() for t in toks if t.strip()]\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(t) < 2: \n",
    "            continue\n",
    "        if re.search(r\"https?://\", t):\n",
    "            continue\n",
    "        if t not in out:\n",
    "            out.append(t)\n",
    "    return \", \".join(out)\n",
    "\n",
    "def launch_labeler(df: pd.DataFrame,\n",
    "                   id_field: str = \"_id\",\n",
    "                   show_fields = (\"title\",\"summary\"),\n",
    "                   autosave_csv: str = AUTOSAVE_CSV):\n",
    "    \"\"\"\n",
    "    sample_df를 순차적으로 라벨링하는 인터랙티브 위젯.\n",
    "    - df: sample_df (cands_yake / cands_keybert / display_text 포함)\n",
    "    - id_field: 문서 고유키 컬럼\n",
    "    - show_fields: 상단에 보여줄 필드들\n",
    "    - autosave_csv: 이동 시 자동 저장되는 경로\n",
    "    \"\"\"\n",
    "    assert id_field in df.columns, f\"{id_field} not in dataframe\"\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    df = _ensure_cols(df)\n",
    "\n",
    "    # 이전 작업 이어붙이기\n",
    "    if os.path.exists(autosave_csv):\n",
    "        try:\n",
    "            prev = pd.read_csv(autosave_csv)\n",
    "            if id_field in prev.columns:\n",
    "                df = df.drop(columns=[c for c in (\"gold_keywords\",\"annotator\",\"notes\") if c in df.columns], errors=\"ignore\") \\\n",
    "                       .merge(prev[[id_field,\"gold_keywords\",\"annotator\",\"notes\"]],\n",
    "                              on=id_field, how=\"left\")\n",
    "                for col in (\"gold_keywords\",\"annotator\",\"notes\"):\n",
    "                    df[col] = df[col].fillna(\"\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] failed to load previous autosave: {e}\")\n",
    "\n",
    "    N = len(df)\n",
    "    idx = 0\n",
    "\n",
    "    # ---- 위젯들 ----\n",
    "    idx_disp = W.BoundedIntText(value=1, min=1, max=max(1, N), description=\"Index\")\n",
    "    progress = W.Label()\n",
    "    id_label = W.HTML()\n",
    "\n",
    "    # 표시 텍스트(가독)\n",
    "    show_areas = []\n",
    "    for f in show_fields:\n",
    "        area = W.HTML(layout=W.Layout(border=\"1px solid #ddd\", padding=\"8px\", height=\"auto\"))\n",
    "        show_areas.append((f, area))\n",
    "\n",
    "    # 후보 선택\n",
    "    def _split_cands(s: str):\n",
    "        return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "\n",
    "    cands_yake_ms   = W.SelectMultiple(options=[], description=\"YAKE\", layout=W.Layout(width=\"48%\", height=\"140px\"))\n",
    "    cands_keybert_ms= W.SelectMultiple(options=[], description=\"KeyBERT\", layout=W.Layout(width=\"48%\", height=\"140px\"))\n",
    "    cands_box = W.HBox([cands_yake_ms, cands_keybert_ms])\n",
    "\n",
    "    # 편집 영역\n",
    "    annotator_tb = W.Text(value=\"\", description=\"Annotator\", layout=W.Layout(width=\"40%\"))\n",
    "    notes_ta     = W.Textarea(value=\"\", description=\"Notes\", layout=W.Layout(width=\"100%\", height=\"80px\"))\n",
    "    gold_ta      = W.Textarea(value=\"\", description=\"Gold Keywords (,로 구분)\", layout=W.Layout(width=\"100%\", height=\"110px\"))\n",
    "\n",
    "    # 버튼들\n",
    "    btn_prev   = W.Button(description=\"◀ Prev\", button_style=\"\")\n",
    "    btn_next   = W.Button(description=\"Next ▶\", button_style=\"primary\")\n",
    "    btn_add_y  = W.Button(description=\"Add YAKE ▶\", button_style=\"\")\n",
    "    btn_add_k  = W.Button(description=\"Add KeyBERT ▶\", button_style=\"\")\n",
    "    btn_clear  = W.Button(description=\"Clear gold\", button_style=\"warning\")\n",
    "    btn_save   = W.Button(description=\"💾 Save\", button_style=\"success\")\n",
    "\n",
    "    msg = W.HTML()\n",
    "\n",
    "    def _update_view():\n",
    "        i = idx_disp.value - 1\n",
    "        row = df.iloc[i]\n",
    "        # 헤더/진행률\n",
    "        id_label.value = f\"<b>{id_field}</b>: {row.get(id_field)} | <b>source</b>: {row.get('source','')}\"\n",
    "        progress.value = f\"{idx_disp.value}/{N}\"\n",
    "\n",
    "        # 본문 표시\n",
    "        for f, area in show_areas:\n",
    "            area.value = f\"<b>{f}</b><br>{(row.get(f) or '')}\"\n",
    "\n",
    "        # 후보 목록\n",
    "        cands_yake_ms.options    = _split_cands(row.get(\"cands_yake\",\"\"))\n",
    "        cands_keybert_ms.options = _split_cands(row.get(\"cands_keybert\",\"\"))\n",
    "\n",
    "        # 편집 값\n",
    "        annotator_tb.value = row.get(\"annotator\",\"\") or \"\"\n",
    "        notes_ta.value     = row.get(\"notes\",\"\") or \"\"\n",
    "        gold_ta.value      = row.get(\"gold_keywords\",\"\") or \"\"\n",
    "\n",
    "        msg.value = \"\"\n",
    "\n",
    "    def _write_back(i):\n",
    "        # normalize하고 df에 되쓰기\n",
    "        df.at[i, \"annotator\"]     = annotator_tb.value.strip()\n",
    "        df.at[i, \"notes\"]         = notes_ta.value.strip()\n",
    "        df.at[i, \"gold_keywords\"] = _normalize_kw_list(gold_ta.value)\n",
    "\n",
    "    def _autosave():\n",
    "        try:\n",
    "            df.to_csv(autosave_csv, index=False)\n",
    "            return True, f\"Saved to {autosave_csv}\"\n",
    "        except Exception as e:\n",
    "            return False, f\"Save failed: {e}\"\n",
    "\n",
    "    def _add_selected(ms_widget):\n",
    "        cur = _normalize_kw_list(gold_ta.value)\n",
    "        cur_set = set([s.strip() for s in cur.split(\",\") if s.strip()]) if cur else set()\n",
    "        add_set = set([s.lower() for s in ms_widget.value])\n",
    "        merged = list(dict.fromkeys([*cur_set, *add_set]))\n",
    "        gold_ta.value = \", \".join(merged)\n",
    "\n",
    "    # 이벤트 바인딩\n",
    "    def on_prev(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i-1 if i>0 else 0)  # 현재 변경사항도 반영되도록\n",
    "        ok, m = _autosave()\n",
    "        idx_disp.value = max(1, i)\n",
    "        _update_view()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    def on_next(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i)\n",
    "        ok, m = _autosave()\n",
    "        idx_disp.value = min(N, i+2)\n",
    "        _update_view()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    def on_jump(change):\n",
    "        # 인덱스 수동 변경 시 현재 변경사항 저장\n",
    "        i = max(0, min(N-1, change[\"old\"]-1))\n",
    "        _write_back(i)\n",
    "        _autosave()\n",
    "        _update_view()\n",
    "\n",
    "    def on_add_y(_): _add_selected(cands_yake_ms)\n",
    "    def on_add_k(_): _add_selected(cands_keybert_ms)\n",
    "    def on_clear(_): gold_ta.value = \"\"\n",
    "    def on_save(_):\n",
    "        i = idx_disp.value - 1\n",
    "        _write_back(i)\n",
    "        ok, m = _autosave()\n",
    "        msg.value = f\"<span style='color:{'green' if ok else 'red'}'>{m}</span>\"\n",
    "\n",
    "    btn_prev.on_click(on_prev)\n",
    "    btn_next.on_click(on_next)\n",
    "    btn_add_y.on_click(on_add_y)\n",
    "    btn_add_k.on_click(on_add_k)\n",
    "    btn_clear.on_click(on_clear)\n",
    "    btn_save.on_click(on_save)\n",
    "    idx_disp.observe(on_jump, names=\"value\")\n",
    "\n",
    "    header = W.HBox([idx_disp, progress])\n",
    "    meta   = W.HTML(\"<b>Document Meta</b>\")\n",
    "    cand_btns = W.HBox([btn_add_y, btn_add_k, btn_clear, btn_save])\n",
    "    nav    = W.HBox([btn_prev, btn_next])\n",
    "\n",
    "    # 레이아웃\n",
    "    box = W.VBox([\n",
    "        header, id_label,\n",
    "        W.HTML(\"<hr>\"),\n",
    "        meta,\n",
    "        *(area for _, area in show_areas),\n",
    "        W.HTML(\"<hr><b>Candidate Keywords</b>\"),\n",
    "        cands_box, cand_btns,\n",
    "        W.HTML(\"<hr><b>Gold Label</b>\"),\n",
    "        annotator_tb, gold_ta, notes_ta,\n",
    "        nav, msg\n",
    "    ])\n",
    "\n",
    "    # 초기화\n",
    "    _update_view()\n",
    "    display(box)\n",
    "\n",
    "    # 반환: 작업본 DataFrame을 호출자도 참조 가능\n",
    "    return df\n",
    "\n",
    "# ---- 사용 예시 ----\n",
    "# 아래 한 줄을 실행하면 위젯이 뜹니다.\n",
    "# sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "# sample_df = _ensure_cols(sample_df)\n",
    "# labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\"title\",\"summary\",\"display_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a04593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_ensure_cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m sample_df = \u001b[43m_ensure_cols\u001b[49m(sample_df)  \u001b[38;5;66;03m# 안전하게 컬럼 준비\u001b[39;00m\n\u001b[32m      3\u001b[39m labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\u001b[33m\"\u001b[39m\u001b[33mtitle\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mdisplay_text\u001b[39m\u001b[33m\"\u001b[39m))\n",
      "\u001b[31mNameError\u001b[39m: name '_ensure_cols' is not defined"
     ]
    }
   ],
   "source": [
    "sample_df = stratified_sample(df, STRATIFY_FIELD, SAMPLE_SIZE).copy()\n",
    "sample_df = _ensure_cols(sample_df)  # 안전하게 컬럼 준비\n",
    "labeled_df = launch_labeler(sample_df, id_field=ID_FIELD, show_fields=(\"title\",\"summary\",\"display_text\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두 명의 CSV를 수집 후 합의 컬럼을 만듭니다. 기본은 교집합+부분합의 규칙을 제공하고, 불일치 건을 표시\n",
    "def parse_kw_list(s: str) -> List[str]:\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return []\n",
    "    # 쉼표/세미콜론 공용 파싱, 소문자/트림\n",
    "    toks = re.split(r\"[;,]\", s)\n",
    "    toks = [re.sub(r\"\\s+\", \" \", t).strip().lower() for t in toks if t.strip()]\n",
    "    # 중복 제거, 길이 2 미만/URL 제거\n",
    "    out = []\n",
    "    for t in toks:\n",
    "        if len(t) < 2: \n",
    "            continue\n",
    "        if re.search(r\"https?://\", t):\n",
    "            continue\n",
    "        if t not in out:\n",
    "            out.append(t)\n",
    "    return out\n",
    "\n",
    "def jaccard(a: List[str], b: List[str]) -> float:\n",
    "    A, B = set(a), set(b)\n",
    "    return len(A & B) / max(1, len(A | B))\n",
    "\n",
    "# A/B 결과 로드\n",
    "A = pd.read_csv(TEMPLATE_A_CSV)\n",
    "B = pd.read_csv(TEMPLATE_B_CSV)\n",
    "\n",
    "# 병합 키: ID_FIELD\n",
    "merged = A.merge(B[[ID_FIELD, \"gold_keywords\"]], on=ID_FIELD, suffixes=(\"_A\", \"_B\"), how=\"outer\")\n",
    "\n",
    "# 합의 규칙\n",
    "final_kw, agree, jacc = [], [], []\n",
    "for _, r in merged.iterrows():\n",
    "    kwsA = parse_kw_list(r.get(\"gold_keywords_A\",\"\"))\n",
    "    kwsB = parse_kw_list(r.get(\"gold_keywords_B\",\"\"))\n",
    "    inter = sorted(set(kwsA) & set(kwsB))\n",
    "    union = sorted(set(kwsA) | set(kwsB))\n",
    "    # 기본 합의안: 교집합이 5개 미만이면 중요한 후보(교집합 + A/B 상위 2개씩)로 제안\n",
    "    if len(inter) >= 5:\n",
    "        fused = inter\n",
    "    else:\n",
    "        fused = sorted(set(inter + kwsA[:2] + kwsB[:2]))\n",
    "    final_kw.append(\", \".join(fused))\n",
    "    agree.append(1 if len(inter) >= max(3, min(len(union)//2, 5)) else 0)\n",
    "    jacc.append(jaccard(kwsA, kwsB))\n",
    "\n",
    "merged[\"gold_keywords_final\"] = final_kw\n",
    "merged[\"annotator_agree_flag\"] = agree       # 1=대체로 합의, 0=재검토 요망\n",
    "merged[\"annotator_jaccard\"]    = jacc\n",
    "\n",
    "merged.to_csv(MERGED_CSV, index=False)\n",
    "merged.head(3)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
